---
title: "Practical Machine Learning - Project Coursera"
author: "Atilio S. Calefi"
date: "26-07-2015"
output: 
  html_document:
    fig_height: 9
    fig_width: 9
---

## Introduction  

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  

In this project, we will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. 

This project was executed into an operating system **Ubuntu 14.04** and **RStudio** with **R version 3.2.1**.

### Preprocessing packages requirements

```{r, echo = T}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(rattle)
```

### Download the Data

If you didn't download the data, this code will do that.

```{r, cache = T}
# Set the appropriate directory setwd()

trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "pml-training.csv"
testFile  <- "pml-testing.csv"

if (!file.exists(trainFile)) {
  download.file(trainUrl, destfile=trainFile)
}
#use method="crurl" deppending of your opperating system
if (!file.exists(testFile)) {
  download.file(testUrl, destfile=testFile)
}
```


###Load the data

This code load the data indo the variables names `training` for the training data set and `testing` for the testing data set.

```{r, cache = T}
training <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv", na.strings=c("NA","#DIV/0!",""))
```

###Set seed for reproducible purpose

To make the results reproducible we use the function `set.seed()`.

```{r, cache=T}
set.seed(12345)
```

### Cleaning the data sets

Lets remove the columns that contain NA values.
```{r, cache = T}
training <- training[, colSums(is.na(training)) == 0] 
testing <- testing[, colSums(is.na(testing)) == 0] 
```  

To construct a data more concise and facilitade the data processing we will remove some obsolete informations.

```{r, cache = T}
classe <- training$classe
trainRemove <- grepl("^X|timestamp|window", names(training))
training <- training[, !trainRemove]
trainCleaned <- training[, sapply(training, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testing))
testing <- testing[, !testRemove]
testCleaned <- testing[, sapply(testing, is.numeric)]
```

The cleaned training data set contains 19622 observations and 60 variables, while the testing data set contains 20 observations and 53 variables.

###Create data partition in the Testing and Training set

The training data will be splited into `70%` just to train the algorithm, and `30%` to validate the process, these `30%` will be used to cross-validation after.

```{r, cache=T}
inTrain <- createDataPartition(y=trainCleaned$classe, p=0.7, list=FALSE)
myTraining <- trainCleaned[inTrain, ]
myTesting <- trainCleaned[-inTrain, ]
dim(myTraining); dim(myTesting)
```

The splip produce a *Training* data set (70%) with `13737` observations and `53` variables and a *Testing* data set (30%) with `5885` observations and `53` variables.

## Data modeling with Radom Tree Forest

Now we have the cleaned data we will proced with the data modeling and validations.
We choose the *Random Forest* algorithm. These technique is robust to correlated covariates. 

```{r, echo=TRUE}
# To cross validate you can use a five-fold in the algorithm.  
# Function to cross validate
#
# controlT <- trainControl(method="cv", repeats = 5)
# FitModel <- randomForest(classe ~ ., data=myTraining, trControl=controlT, ntree = 250)
#

FitModel <- randomForest(classe ~ ., data=myTraining)
FitModel
```

See the decision tree in *Appendix* **Figure 1** and Correlation matrix in the **Figure 2**.

The codes above estimate the model performace on the validation data set.

```{r, echo=TRUE}
predictions1st <- predict(FitModel, myTesting, type = "class")
confusionMatrix(predictions1st, myTesting$classe)
```

Vizualising the model accuracy and the estimated error.

```{r, cache = T}
Accuracy <- postResample(predictions1st, myTesting$classe)
Accuracy

EstimateError <- 1 - as.numeric(confusionMatrix(myTesting$classe, predictions1st)$overall[1])
EstimateError
```

So, the estimated accuracy of the model is `99.27%` and the estimated out-of-sample error is `0.73%`.

The following formula presented the better prediction model. We used that model to the submission project.

```{r, echo=TRUE}
predictionsFinal <- predict(FitModel, myTesting, type = "class")
```

## Appendix

**Figure 1.** Decision tree.

```{r, echo=TRUE}
FitModel1 <- rpart(classe ~ ., data = myTraining, method="class")
fancyRpartPlot(FitModel1)
```

**Figure 2.** Correlation matrix  
```{r, cache = T}
library(corrplot)
Plot <- cor(myTraining[, -length(names(myTraining))])
corrplot(Plot, method="color")
```
